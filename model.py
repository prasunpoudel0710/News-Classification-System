# -*- coding: utf-8 -*-
"""News Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nmdzI5_cwAsir4_WxyJOP1nHlVpo7xT8
"""
from sklearn import preprocessing
import pandas as pd
import matplotlib.pyplot as plt
import pickle
import seaborn as sns
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from nltk.tokenize import word_tokenize

import re
import warnings

warnings.filterwarnings("ignore")

nltk.download('stopwords')
nltk.download('punkt')

data = pd.read_excel("C:/finalproject/data2.xlsx")

data.head()

"""# Understanding Features and Target Variables"""

data['Category'].unique()

print(data.shape)

print(data.dtypes)

"""# Checking for NULL values"""

print(data.isnull().any())

data['News_length'] = data['Text'].str.len()
print(data['News_length'])

"""# Distribution Plot"""

sns.distplot(data['News_length']).set_title('News length distribution')

"""# WordCloud"""


def create_wordcloud(words):
    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(words)
    plt.figure(figsize=(10, 7))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis('off')
    plt.show()


# subset = data[data.Category == "business"]
# text = subset.Text.values
# words = " ".join(text)
# create_wordcloud(words)
#
# subset = data[data.Category == "entertainment"]
# text = subset.Text.values
# words = " ".join(text)
# create_wordcloud(words)
#
# subset = data[data.Category == "politics"]
# text = subset.Text.values
# words = " ".join(text)
# create_wordcloud(words)
#
# subset = data[data.Category == "sport"]
# text = subset.Text.values
# words = " ".join(text)
# create_wordcloud(words)
#
# subset = data[data.Category == "tech"]
# text = subset.Text.values
# words = " ".join(text)
# create_wordcloud(words)

"""# Feature Engineering

Removing the special characters<br>
1) \r<br>
2) \n<br>

Removing Punctuations and Stopwords
"""


def process_text(text):
    text = text.lower().replace('\n', ' ').replace('\r', '').strip()
    text = re.sub(' +', ' ', text)
    text = re.sub(r'[^\w\s]', '', text)

    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)

    filtered_sentence = [w for w in word_tokens if w not in stop_words]

    text = " ".join(filtered_sentence)
    return text


print(data.head())

"""# Label Encoding"""


def preprocess(data):
    label_encoder = preprocessing.LabelEncoder()
    data['Category_target'] = label_encoder.fit_transform(data['Category'])
    data['Text_parsed'] = data['Text'].apply(process_text)
    return data


data = preprocess(data)

print(data.head())

print(data.Category)

print(data['Category_target'].unique())

print(data['Category'].unique())

"""# Split the data in Training and testing"""


def train_model(data):
    data = preprocess(data)
    X_train, X_test, y_train, y_test = train_test_split(data['Text_parsed'],
                                                        data['Category_target'],
                                                        test_size=0.2,
                                                        random_state=8)

    ngram_range = (1, 2)
    min_df = 10
    max_df = 1.
    max_features = 300

    tfidf = TfidfVectorizer(encoding='utf-8',
                            ngram_range=ngram_range,
                            stop_words=None,
                            lowercase=False,
                            max_df=max_df,
                            min_df=min_df,
                            max_features=max_features,
                            norm='l2',
                            sublinear_tf=True)

    features_train = tfidf.fit_transform(X_train).toarray()
    labels_train = y_train
    print(features_train)

    features_test = tfidf.transform(X_test).toarray()
    labels_test = y_test
    print(features_test.shape)

    """# Building model """

    """# Logistic Regression """

    global model
    global model_predictions
    model = LogisticRegression()  # multi_class='ovr' solver='liblinear'->one vs all || multi_class='multinomial' solver='lbfgs'
    print(model.get_params())
    model.fit(features_train, labels_train)
    model_predictions = model.predict(features_test)
    print('Accuracy of training model: ', model.score(features_train, labels_train))
    print('Accuracy of testing model: ', accuracy_score(labels_test, model_predictions))

    """# Hyper Parameter Tuning (Logistic Regression)"""

    model = LogisticRegression(C=1, warm_start=True)
    model.fit(features_train, labels_train)
    model_predictions = model.predict(features_test)
    print('Accuracy: ', accuracy_score(labels_test, model_predictions))
    print(classification_report(labels_test, model_predictions))
    return model, tfidf, labels_test, model_predictions


model, tfidf, labels_test, model_predictions = train_model(data)

"""Accuracy remains the same after hyper parameter tuning"""


def cm():
    Confuse_matrix = confusion_matrix(labels_test, model_predictions)
    return Confuse_matrix


def Input(inputText, label):
    incoming_data = {'Text': inputText, 'Category': label}

    """Data feeding for minimizing over fitting"""
    incoming_df = pd.DataFrame(incoming_data, index=[0])
    new_data = data.append(incoming_df, ignore_index=True)
    new_data.to_excel('data2.xlsx', index=False)
    model = train_model(new_data)
    filename = 'model.pkl'
    pickle.dump(model, open(filename, 'wb'))
    print("model is retained")
    print(new_data)
    return incoming_data


filename1 = 'Overall.pkl'
pickle.dump((tfidf, cm, Input), open(filename1, 'wb'))
filename2 = 'model.pkl'
pickle.dump(model, open(filename2, 'wb'))

# def classify_text(text, true_label):
#     # Preprocess the text
#     processed_text = process_text(text)
#
#     # Vectorize the text using the same TfidfVectorizer as in the training
#     text_vector = tfidf.transform([processed_text]).toarray()
#
#     # Predict the category using the trained Logistic Regression model
#     category_idx = model.predict(text_vector)
#
#     # Map the category index to its label using the LabelEncoder
#     category_label = label_encoder.inverse_transform([category_idx])[0]
#
#     if true_label == category_label:
#         print("predicted correctly")
#     else:
#         print("predicted incorrectly")
#
#     return category_label
#
#
# Text = ""
# Category = "politics"  # business entertainment politics sport tech
# data = Input(Text, Category)  # data={'inputText': inputText, 'label': label}
#
# print("prediction:", classify_text(data['Text'], data['Category']))
#
#
# def Accuracy():
#     return round(accuracy_score(labels_test, model_predictions) * 100, 2)
#
#
# confmatr = confusion_matrix(labels_test, model_predictions)
# sns.heatmap(confmatr, annot=True, cmap='Blues')
# plt.xlabel("model_predictions")
# plt.ylabel("features_test")
# plt.show()
# print(cm)
